{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS9bn7bhqpCG",
    "outputId": "38bcc44c-8019-449a-e1ee-2de1e086c68e"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iD0I8HwdqpCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aFY-8_lrqpCg"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fsRv8kuNqpCw",
    "outputId": "773d68d4-3c4e-42e4-c9ca-be5c60ac5412"
   },
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.01)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58399792e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926563e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75109456e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgjOFB8kqpC_"
   },
   "source": [
    "### Hiperpar치metros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABViOAgTqpDI"
   },
   "outputs": [],
   "source": [
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([1, 4, 8]))\n",
    "\n",
    "\n",
    "HP_LAYERS =    hp.HParam('layers', hp.Discrete([2, 3, 4]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([50, 100, 200]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([1e-5,1e-4,1e-3]))\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min',\n",
    "                                   min_delta=0,\n",
    "                                   patience=6,\n",
    "                                   restore_best_weights=True)]\n",
    "# batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lXRw_6G8qpDL"
   },
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning2').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS,\n",
    "                 HP_NUM_UNITS,\n",
    "                 HP_LEARNING, \n",
    "                 HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6JG3WeEOqpDY"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS], input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams[HP_BATCHSIZE], shuffle=True, verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fnLlAKGuqpDp"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tu8q13lqpDs",
    "outputId": "80f9eb4f-dc3a-445e-ff6d-3d42dcb085c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 1}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 1ms/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
      "Loss: 0.002899657469242811 Tiempo transcurrido: 1163.5126078128815\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 799us/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Loss: 0.0037683527916669846 Tiempo transcurrido: 317.6881787776947\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 688us/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Loss: 0.00425115367397666 Tiempo transcurrido: 152.47157073020935\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 1}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 978us/step - loss: 3.2908e-04 - mean_squared_error: 3.2908e-04\n",
      "Loss: 0.00032908254070207477 Tiempo transcurrido: 975.9348285198212\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 710us/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "Loss: 0.0013510133139789104 Tiempo transcurrido: 283.6802923679352\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 762us/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.001956339692696929 Tiempo transcurrido: 133.7034547328949\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 1}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 1ms/step - loss: 3.8638e-04 - mean_squared_error: 3.8638e-04\n",
      "Loss: 0.0003863837046083063 Tiempo transcurrido: 762.9068846702576\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 954us/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
      "Loss: 0.0013102213852107525 Tiempo transcurrido: 93.69166350364685\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 837us/step - loss: 6.6553e-04 - mean_squared_error: 6.6553e-04\n",
      "Loss: 0.0006655268953181803 Tiempo transcurrido: 159.56621098518372\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 1}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "Loss: 0.002557197352871299 Tiempo transcurrido: 1182.9961924552917\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 759us/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Loss: 0.0036436552181839943 Tiempo transcurrido: 352.480993270874\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 982us/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Loss: 0.004320705775171518 Tiempo transcurrido: 174.75655841827393\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 1}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "193/193 [==============================] - 0s 1ms/step - loss: 2.5843e-04 - mean_squared_error: 2.5843e-04\n",
      "Loss: 0.00025843409821391106 Tiempo transcurrido: 1252.9006004333496\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "datos = []\n",
    "\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "            for batch_size in HP_BATCHSIZE.domain.values:\n",
    "                t = time.time()\n",
    "                hparams = {\n",
    "\n",
    "                    HP_LAYERS: deep_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_BATCHSIZE: batch_size,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('\\n--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                score = run('logs/hparam_tuning2/' + run_name, hparams)\n",
    "                t = time.time()-t\n",
    "                session_num += 1\n",
    "                print(\"Loss:\", score, \"Tiempo transcurrido:\", t)\n",
    "            \n",
    "            datos.append([deep_layers, num_units, learning_rate, batch_size, score, t])\n",
    "\n",
    "print(session_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1ct8OIfqpD3"
   },
   "source": [
    "### Guardar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6xZZqEhqpD5"
   },
   "outputs": [],
   "source": [
    "filename = \"historial_ecsdiff_tunning.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep size\", \"Num units\", \"Learning rate\", \"MSE\", \"Tiempo de ejecuci칩n\"])\n",
    "\n",
    "df.sort_values(by=[\"MSE\", \"Tiempo de ejecuci칩n\"], ascending=[0,0], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "z9BerX2yqpD-",
    "outputId": "992e08c9-0adb-4f57-e50e-df65c91fe142",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ3JJWuZqpEB",
    "outputId": "230a8ed2-6429-49df-f611-d68381c31fc3"
   },
   "outputs": [],
   "source": [
    "np.sum(df[[\"Tiempo de ejecuci칩n\"]])/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh9rbb8kqpED"
   },
   "outputs": [],
   "source": [
    "# rm -rf /tmp/tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm044iKXqpEM"
   },
   "source": [
    "### Now in terminal:\n",
    "`python3 -m tensorboard.main --logdir='/home/isidro/Documents/github/neurapprox/logs/hparam_tuning'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 7439\n",
    "%tensorboard --logdir logs/hparam_tuning2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tunning_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
